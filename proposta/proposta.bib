@article{Abeliuk2013,
abstract = {The suffix tree is an extremely important data structure for stringology, with a wealth of applications in bioinformatics. Classical implementations require much space, which renders them useless for large problems. Recent research has yielded two implementations offering widely different space-time tradeoffs. However, each of them has practicality problems regarding either space or time requirements. In this paper we implement a recent theoretical proposal and show it yields an extremely interesting structure that lies in between, offering both practical times and affordable space. The implementation is by no means trivial and involves significant algorithm engineering. {\textcopyright} Springer-Verlag Berlin Heidelberg 2010.},
author = {Abeliuk, Andr{\'{e}}s and C{\'{a}}novas, Rodrigo and Navarro, Gonzalo},
doi = {10.3390/a6020319},
file = {:home/paulo/Documents/work/bib/files/Abeliuk, C{\'{a}}novas, Navarro - 2013 - Practical compressed suffix trees.pdf:pdf},
isbn = {3642131921},
issn = {19994893},
journal = {Algorithms},
keywords = {Bioinformatics,Compressed data structures,Repetitive sequence collections,Suffix trees},
number = {2},
pages = {319--351},
title = {{Practical compressed suffix trees}},
volume = {6},
year = {2013}
}
@inproceedings{Apostolico1985,
author = {Apostolico, A},
booktitle = {Combinatorial Algorithms on Words},
file = {:home/paulo/Documents/work/bib/files/Apostolico - 1985 - The myriad virtues of subword trees.pdf:pdf},
keywords = {dblp},
pages = {85--96},
title = {{The myriad virtues of subword trees.}},
volume = {12},
year = {1985}
}
@inproceedings{Belazzougui2016,
author = {Belazzougui, Djamal and Gagie, Travis and M{\"{a}}kinen, Veli and Previtali, Marco and Puglisi, Simon J},
booktitle = {LATIN 2016: Theoretical Informatics, 12th Latin American Symposium},
doi = {10.1007/978-3-662-49529-2},
file = {:home/paulo/Documents/work/bib/files/Belazzougui et al. - 2016 - Bidirectional variable-order de Bruijn Graphs.pdf:pdf},
isbn = {978-3-662-49528-5},
pages = {164--178},
title = {{Bidirectional variable-order de Bruijn Graphs}},
url = {http://link.springer.com/10.1007/978-3-662-49529-2},
year = {2016}
}
@article{Bloom1970,
abstract = {In this paper trade-offs among certain computational factors in hash coding are analyzed. The paradigm problem considered is that of testing a series of messages one-by-one for membership in a given set of messages. Two new hash-coding methods are examined and compared with a particular conventional hash-coding method. The computational factors considered are the size of the hash area (space), the time required to identify a message as a nonmember of the given set (reject time), and an allowable error frequency. The new methods are intended to reduce the amount of space required to contain the hash-coded information from that associated with conventional methods. The reduction in space is accomplished by exploiting the possibility that a small fraction of errors of commission may be tolerable in some applications, in particular, applications in which a large amount of data is involved and a core resident hash area is consequently not feasible using conventional methods. In such applications, it is envisaged that overall performance could be improved by using a smaller core resident hash area in conjunction with the new methods and, when necessary, by using some secondary and perhaps time-consuming test to “catch” the small fraction of errors associated with the new methods. An example is discussed which illustrates possible areas of application for the new methods. Analysis of the paradigm problem demonstrates that allowing a small number of test messages to be falsely identified as members of the given set will permit a much smaller hash area to be used without increasing reject time.},
author = {Bloom, Burton H},
doi = {10.1145/362686.362692},
isbn = {0001-0782},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {and phrases,hash addressing,hash coding,retrieval efficiency,retrieval trade-offs,scatter storage,searching,storage,storage layout},
number = {7},
pages = {422--426},
title = {{Space/time trade-offs in hash coding with allowable errors}},
volume = {13},
year = {1970}
}
@inproceedings{Boucher2015,
abstract = {The de Bruijn graph {\$}G{\_}K{\$} of a set of strings {\$}S{\$} is a key data structure in genome assembly that represents overlaps between all the {\$}K{\$}-length substrings of {\$}S{\$}. Construction and navigation of the graph is a space and time bottleneck in practice and the main hurdle for assembling large, eukaryote genomes. This problem is compounded by the fact that state-of-the-art assemblers do not build the de Bruijn graph for a single order (value of {\$}K{\$}) but for multiple values of {\$}K{\$}. More precisely, they build {\$}d{\$} de Bruijn graphs, each with a specific order, i.e., {\$}G{\_}{\{}K{\_}1{\}}, G{\_}{\{}K{\_}2{\}}, ..., G{\_}{\{}K{\_}d{\}}{\$}. Although, this paradigm increases the quality of the assembly produced, it increases the memory by a factor of {\$}d{\$} in most cases. In this paper, we show how to augment a succinct de Bruijn graph representation by Bowe et al. (Proc. WABI, 2012) to support new operations that let us change order on the fly, effectively representing all de Bruijn graphs of order up to some maximum {\$}K{\$} in a single data structure. Our experiments show our variable-order de Bruijn graph only modestly increases space usage, construction time, and navigation time compared to a single order graph.},
address = {Snowbird},
archivePrefix = {arXiv},
arxivId = {1411.2718},
author = {Boucher, Christina and Bowe, Alex and Gagie, Travis and Puglisi, Simon J. and Sadakane, Kunihiko},
booktitle = {Proceedings of the 2015 Data Compression Conference - DCC 2015},
doi = {10.1109/DCC.2015.70},
eprint = {1411.2718},
file = {:home/paulo/Documents/work/bib/files/Boucher et al. - 2015 - Variable-Order de Bruijn Graphs.pdf:pdf},
isbn = {9781479984305},
issn = {10680314},
pages = {383--392},
title = {{Variable-Order de Bruijn Graphs}},
year = {2015}
}
@inproceedings{Bowe2012,
abstract = {We propose a new succinct de Bruijn graph representation. If the de Bruijn graph of k -mers in a DNA sequence of length N has m edges, it can be represented in 4 m + o ( m ) bits. This is much smaller than existing ones. The numbers of outgoing and incoming edges of a node are computed in constant time, and the outgoing and incoming edge with given label are found in constant time and O ( k ) time, respectively. The data structure is constructed in O ( Nk log m/ log log m ) time using no additional space.},
address = {Ljubljana},
annote = {NULL},
author = {Bowe, Alexander and Onodera, Taku and Sadakane, Kunihiko and Shibuya, Tetsuo},
booktitle = {Proceedings of the Workshop on Algorithms in Bioinformatics - WABI 2012},
doi = {10.1007/978-3-642-33122-0_18},
file = {:home/paulo/Documents/work/bib/files/Bowe et al. - 2012 - Succinct de Bruijn graphs.pdf:pdf},
isbn = {9783642331213},
issn = {03029743},
pages = {225--235},
publisher = {Springer, Berlin, Heidelberg},
title = {{Succinct de Bruijn graphs}},
url = {http://link.springer.com/10.1007/978-3-642-33122-0{\_}18},
year = {2012}
}
@article{Burrows1994,
abstract = {We describe a block-sorting, lossless data compression algorithm, and our implementation of that algorithm. We compare the performance of our implementation with widely available data compressors running on the same hardware. The algorithm works by applying a reversible transformation to a block of input text. The transformation does not itself compress the data, but reorders it to make it easy to compress with simple algorithms such as move-to-front coding. Our algorithm achieves speed comparable to algorithms based on the techniques of Lempel and Ziv, but obtains compression close to the best statistical modelling techniques. The size of the input block must be large (a few kilobytes) to achieve good compression.},
archivePrefix = {arXiv},
arxivId = {0908.0239},
author = {Burrows, Michael and Wheeler, David J},
doi = {10.1.1.37.6774},
eprint = {0908.0239},
isbn = {0769518966},
issn = {15708667},
journal = {Systems Research},
number = {124},
pages = {24},
title = {{A block-sorting lossless data compression algorithm}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.141.5254},
volume = {R},
year = {1994}
}
@article{Butler2008,
abstract = {New {\{}DNA{\}} sequencing technologies deliver data at dramatically lower costs but demand new analytical methods to take full advantage of the very short reads that they produce. We provide an initial, theoretical solution to the challenge of de novo assembly from whole-genome shotgun “microreads.” For 11 genomes of sizes up to 39 Mb, we generated high-quality assemblies from 80× coverage by paired 30-base simulated reads modeled after real Illumina-Solexa reads. The bacterial genomes of Campylobacter jejuni and Escherichia coli assemble optimally, yielding single perfect contigs, and larger genomes yield assemblies that are highly connected and accurate. Assemblies are presented in a graph form that retains intrinsic ambiguities such as those arising from polymorphism, thereby providing information that has been absent from previous genome assemblies. For both C. jejuni and E. coli, this assembly graph is a single edge encompassing the entire genome. Larger genomes produce more complicated graphs, but the vast majority of the bases in their assemblies are present in long edges that are nearly always perfect. We describe a general method for genome assembly that can be applied to all types of {\{}DNA{\}} sequence data, not only short read data, but also conventional sequence reads.},
annote = {{\{}PMID:{\}} 18340039},
author = {Butler, Jonathan and MacCallum, Iain and Kleber, Michael and Shlyakhter, Ilya A and Belmonte, Matthew K and Lander, Eric S and Nusbaum, Chad and Jaffe, David B},
doi = {10.1101/gr.7337908},
file = {:home/paulo/Documents/work/bib/files/Butler et al. - 2008 - ALLPATHS De novo assembly of whole-genome shotgun microreads.pdf:pdf},
issn = {1088-9051, 1549-5469},
journal = {Genome Research},
number = {5},
pages = {810--820},
shorttitle = {ALLPATHS},
title = {{ALLPATHS: De novo assembly of whole-genome shotgun microreads}},
url = {http://genome.cshlp.org/content/18/5/810},
volume = {18},
year = {2008}
}
@article{Cazaux2016,
author = {Cazaux, Bastien and Lecroq, Thierry and Rivals, Eric},
doi = {10.1016/j.jcss.2016.06.008},
file = {:home/paulo/Documents/work/bib/files/Cazaux, Lecroq, Rivals - 2016 - Linking indexing data structures to de Bruijn graphs Construction and update.pdf:pdf},
issn = {00220000},
journal = {Journal of Computer and System Sciences},
month = {jul},
title = {{Linking indexing data structures to de Bruijn graphs: Construction and update}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0022000016300502},
volume = {In press},
year = {2016}
}
@article{Chaisson2008,
abstract = {In the last year, high-throughput sequencing technologies have progressed from proof-of-concept to production quality. While these methods produce high-quality reads, they have yet to produce reads comparable in length to Sanger-based sequencing. Current fragment assembly algorithms have been implemented and optimized for mate-paired Sanger-based reads, and thus do not perform well on short reads produced by short read technologies. We present a new Eulerian assembler that generates nearly optimal short read assemblies of bacterial genomes and describe an approach to assemble reads in the case of the popular hybrid protocol when short and long Sanger-based reads are combined.},
annote = {{\{}PMID:{\}} 18083777},
author = {Chaisson, Mark J and Pevzner, Pavel A},
doi = {10.1101/gr.7088808},
file = {:home/paulo/Documents/work/bib/files/Chaisson, Pevzner - 2008 - Short read fragment assembly of bacterial genomes.pdf:pdf},
issn = {1088-9051, 1549-5469},
journal = {Genome Research},
number = {2},
pages = {324--330},
title = {{Short read fragment assembly of bacterial genomes}},
url = {http://genome.cshlp.org/content/18/2/324},
volume = {18},
year = {2008}
}
@article{Chikhi2015,
abstract = {The de Bruijn graph plays an important role in bioinformatics, especially in the context of de novo assembly. However, the representation of the de Bruijn graph in memory is a computational bottleneck for many assemblers. Recent papers proposed a navigational data structure approach in order to improve memory usage. We prove several theoretical space lower bounds to show the limitation of these types of approaches. We further design and implement a general data structure (DBGFM) and demonstrate its use on a human whole-genome dataset, achieving space usage of 1.5 GB and a 46{\%} improvement over previous approaches. As part of DBGFM, we develop the notion of frequency-based minimizers and show how it can be used to enumerate all maximal simple paths of the de Bruijn graph using only 43 MB of memory. Finally, we demonstrate that our approach can be integrated into an existing assembler by modifying the ABySS software to use DBGFM.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1401.5383},
author = {Chikhi, Rayan and Limasset, Antoine and Jackman, Shaun and Simpson, Jared T and Medvedev, Paul},
doi = {10.1089/cmb.2014.0160},
eprint = {1401.5383},
file = {:home/paulo/Documents/work/bib/files/Chikhi et al. - 2015 - On the Representation of De Bruijn Graphs.pdf:pdf},
isbn = {9783319052687},
issn = {1066-5277},
journal = {Journal of Computational Biology},
month = {may},
number = {5},
pages = {336--352},
pmid = {25629448},
publisher = {Mary Ann Liebert, Inc. 140 Huguenot Street, 3rd Floor New Rochelle, NY 10801 USA},
title = {{On the Representation of De Bruijn Graphs}},
url = {http://online.liebertpub.com/doi/10.1089/cmb.2014.0160},
volume = {22},
year = {2015}
}
@article{Chikhi2013,
abstract = {BACKGROUND: The de Bruijn graph data structure is widely used in next-generation sequencing (NGS). Many programs, e.g. de novo assemblers, rely on in-memory representation of this graph. However, current techniques for representing the de Bruijn graph of a human genome require a large amount of memory (≥30 GB).$\backslash$n$\backslash$nRESULTS: We propose a new encoding of the de Bruijn graph, which occupies an order of magnitude less space than current representations. The encoding is based on a Bloom filter, with an additional structure to remove critical false positives.$\backslash$n$\backslash$nCONCLUSIONS: An assembly software implementing this structure, Minia, performed a complete de novo assembly of human genome short reads using 5.7 GB of memory in 23 hours.},
annote = {NULL},
author = {Chikhi, Rayan and Rizk, Guillaume},
doi = {10.1007/978-3-642-33122-0_19},
file = {:home/paulo/Documents/work/bib/files/Chikhi, Rizk - 2012 - Space-efficient and exact de Bruijn graph representation based on a Bloom Filter.pdf:pdf},
isbn = {9783642331213},
issn = {03029743},
journal = {Algorithms for Molecular Biology},
keywords = {bloom filter,de bruijn graph,de novo assembly},
pages = {22},
pmid = {24040893},
title = {{Space-efficient and exact de Bruijn graph representation based on a Bloom Filter}},
volume = {8},
year = {2013}
}
@article{Compeau2011,
abstract = {A mathematical concept known as a de Bruijn graph turns the formidable challenge of assembling a contiguous genome from billions of short sequencing reads into a tractable computational problem.},
archivePrefix = {arXiv},
arxivId = {doi:10.1038/nbt.2023},
author = {Compeau, Phillip E C and Pevzner, Pavel A and Tesler, Glenn},
doi = {10.1038/nbt.2023},
eprint = {nbt.2023},
file = {:home/paulo/Documents/work/bib/files/Compeau, Pevzner, Tesler - 2011 - How to apply de Bruijn graphs to genome assembly.pdf:pdf},
isbn = {0000110010111},
issn = {1087-0156},
journal = {Nature Biotechnology},
number = {11},
pages = {987--991},
pmid = {22068540},
primaryClass = {doi:10.1038},
title = {{How to apply de Bruijn graphs to genome assembly}},
url = {http://dx.doi.org/10.1038/nbt.2023},
volume = {29},
year = {2011}
}
@article{Conway2011,
abstract = {Motivation: Second-generation sequencing technology makes it feasible for many researches to obtain enough sequence reads to attempt the de novo assembly of higher eukaryotes (including mammals). De novo assembly not only provides a tool for understanding wide scale biological variation, but within human biomedicine, it offers a direct way of observing both large-scale structural variation and fine-scale sequence variation. Unfortunately, improvements in the computational feasibility for de novo assembly have not matched the improvements in the gathering of sequence data. This is for two reasons: the inherent computational complexity of the problem and the in-practice memory requirements of tools. Results: In this article, we use entropy compressed or succinct data structures to create a practical representation of the de Bruijn assembly graph, which requires at least a factor of 10 less storage than the kinds of structures used by deployed methods. Moreover, because our representation is entropy compressed, in the presence of sequencing errors it has better scaling behaviour asymptotically than conventional approaches. We present results of a proof-of-concept assembly of a human genome performed on a modest commodity server. Availability: Binaries of programs for constructing and traversing the de Bruijn assembly graph are available from},
archivePrefix = {arXiv},
arxivId = {1008.2555},
author = {Conway, Thomas C and Bromage, Andrew J},
doi = {10.1093/bioinformatics/btq697},
eprint = {1008.2555},
file = {:home/paulo/Documents/work/bib/files/Conway, Bromage - 2011 - Succinct data structures for assembling large genomes.pdf:pdf},
isbn = {1367-4811},
issn = {13674803},
journal = {Bioinformatics},
month = {feb},
number = {4},
pages = {479--486},
pmid = {21245053},
publisher = {Oxford University Press},
title = {{Succinct data structures for assembling large genomes}},
url = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btq697},
volume = {27},
year = {2011}
}
@article{Farina2009,
abstract = {In this paper, we study different approaches for rank and select on sequences of bytes and propose new implementation strategies. Extensive experimental evaluation comparing the efficiency of the different alternatives are provided. Given a sequence of bits, a rank query counts the number of occurrences of the bit 1 up to a given position, and a select query returns the position of the ith occurrence of the bit 1. These operations are widely used in information retrieval and management, being the base of several data structures and algorithms for text collections, graphs, etc. There exist solutions for computing these operations on sequences of bits in constant time using additional information. However, new applications require rank and select to be computed on sequences of bytes instead of bits. The solutions for the binary case are not directly applicable to sequences of bytes. The existing solutions for the byte case vary in their space-time trade-off which can still be improved. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
author = {Fari{\~{n}}a, Antonio and Ladra, Susana and Pedreira, Oscar and Places, {\'{A}}ngeles S.},
doi = {10.1016/j.entcs.2009.03.019},
file = {:home/paulo/Documents/work/bib/files/Fari{\~{n}}a et al. - 2009 - Rank and Select for Succinct Data Structures.pdf:pdf},
issn = {15710661},
journal = {Electronic Notes in Theoretical Computer Science},
keywords = {algorithms,information retrieval,rank,select,succinct data structures},
number = {C},
pages = {131--145},
title = {{Rank and Select for Succinct Data Structures}},
volume = {236},
year = {2009}
}
@article{Ferragina2009,
author = {Ferragina, Paolo and Giancarlo, Raffaele and Manzini, Giovanni},
doi = {10.1016/j.ic.2008.12.010},
issn = {08905401},
journal = {Information and Computation},
number = {8},
pages = {849--866},
title = {{The myriad virtues of Wavelet Trees}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0890540108001594},
volume = {207},
year = {2009}
}
@inproceedings{Ferragina2000,
abstract = {We address the issue of compressing and indexing data. We devise a data structure whose space occupancy is a function of the entropy of the underlying data set. We call the data structure opportunistic since its space occupancy is decreased when the input is compressible and this space reduction is achieved at no significant slowdown in the query performance. More precisely, its space occupancy is optimal in an information-content sense because text T[1,u] is stored using O(Hk (T))+o(1) bits per input symbol in the worst case, where Hk (T) is the kth order empirical entropy of T (the bound holds for any fixed k). Given an arbitrary string P[1,p], the opportunistic data structure allows to search for the occurrences of P in T in O(p+occlog $\epsilon$u) time (for any fixed $\epsilon${\textgreater}0). If data are uncompressible we achieve the best space bound currently known (Grossi and Vitter, 2000); on compressible data our solution improves the succinct suffix array of (Grossi and Vitter, 2000) and the classical suffix tree and suffix array data structures either in space or in query time or both. We also study our opportunistic data structure in a dynamic setting and devise a variant achieving effective search and update time bounds. Finally, we show how to plug our opportunistic data structure into the Glimpse tool (Manber and Wu, 1994). The result is an indexing tool which achieves sublinear space and sublinear query time complexity},
address = {Redondo Beach},
author = {Ferragina, Paolo and Manzini, Giovanni},
booktitle = {Proceedings 41st Annual Symposium on Foundations of Computer Science},
doi = {10.1109/SFCS.2000.892127},
file = {:home/paulo/Documents/work/bib/files/Ferragina, Manzini - 2000 - Opportunistic data structures with applications.pdf:pdf},
isbn = {0-7695-0850-2},
issn = {0272-5428},
keywords = {Computer science,Costs,Data engineering,Data structures,Entropy,Fault tolerance,Glimpse tool,Indexing,Plugs,Postal services,Tree data structures,computational complexity,data compression,data indexing,data set,data structures,database indexing,database theory,entropy,opportunistic data structures,query performance,search,sublinear query time complexity,sublinear space complexity,succinct suffix array,suffix array data structures,suffix tree data structures},
pages = {390--398},
title = {{Opportunistic data structures with applications}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=892127},
year = {2000}
}
@inproceedings{Grossi2003,
abstract = {We present a novel implementation of compressed su{\~{}}x arrays exhibiting new tradeoffs between search time and space occupancy for a given text (or sequence) of n sym-bols over an alphabet E, where each symbol is encoded by lg ]E I bits. We show that compressed su1{\~{}}x arrays use just nHh + O(n lglg n{\~{}} lgl{\~{}} I n) bits, while retaining full text in-dexing functionalities, such as searching any pattern se-quence of length m in O(mlg [E[ + polylog(n)) time. The term Hh {\textless} lg IEI denotes the hth-order empirical entropy of the text, which means that our index is nearly optimal in space apart from lower-order terms, achieving asymptotically the empirical entropy of the text (with a multiplicative con-stant 1). If the text is highly compressible so that H{\~{}} = o(1) and the alphabet size is small, we obtain a text index with o(m) search time that requires only o(n) bits. Further results and tradeoffs are reported in the paper.},
address = {Philadelphia},
author = {Grossi, Roberto and Gupta, Ankur and Vitter, Jeffrey Scott},
booktitle = {Proceedings of the fourteenth annual ACM-SIAM symposium on discrete algorithms - SODA 2003},
file = {:home/paulo/Documents/work/bib/files/Grossi, Gupta, Vitter - 2003 - High-Order Entropy-Compressed Text Indexes.pdf:pdf},
pages = {841--850},
title = {{High-Order Entropy-Compressed Text Indexes}},
year = {2003}
}
@phdthesis{Jacobson1989,
author = {Jacobson, Guy},
school = {Carnegie Mellon University},
title = {{Succinct static data structures}},
year = {1989}
}
@book{Jones2004,
abstract = {This introductory text offers a clear exposition of the algorithmic principles driving advances in bioinformatics. Accessible to students in both biology and computer science, it strikes a unique balance between rigorous mathematics and practical techniques, emphasizing the ideas underlying algorithms rather than offering a collection of apparently unrelated {\{}problems.The{\}} book introduces biological and algorithmic ideas together, linking issues in computer science to biology and thus capturing the interest of students in both subjects. It demonstrates that relatively few design techniques can be used to solve a large number of practical problems in biology, and presents this material {\{}intuitively.An{\}} Introduction to Bioinformatics Algorithms is one of the first books on bioinformatics that can be used by students at an undergraduate level. It includes a dual table of contents, organized by algorithmic idea and biological idea; discussions of biologically relevant problems, including a detailed problem formulation and one or more solutions for each; and brief biographical sketches of leading figures in the field. These interesting vignettes offer students a glimpse of the inspirations and motivations for real work in bioinformatics, making the concepts presented in the text more concrete and the techniques more {\{}approachable.PowerPoint{\}} presentations, practical bioinformatics problems, sample code, diagrams, demonstrations, and other materials can be found at the Author's website.},
author = {Jones, Neil C and Pevzner, Pavel},
isbn = {9780262101066},
keywords = {Computers / Bioinformatics,Computers / Computer Science,Computers / Programming / Algorithms,Science / Life Sciences / Molecular Biology},
publisher = {MIT Press},
title = {{An Introduction to Bioinformatics Algorithms}},
year = {2004}
}
@article{Lander2001,
abstract = {The human genome holds an extraordinary trove of information about human development, physiology, medicine and evolution. Here we report the results of an international collaboration to produce and make freely available a draft sequence of the human genome. We also present an initial analysis of the data, describing some of the insights that can be gleaned from the sequence.},
author = {Lander, Eric S and Linton, Lauren M and Birren, Bruce and Nusbaum, Chad and Zody, Michael C and Baldwin, Jennifer and Devon, Keri and Dewar, Ken and Doyle, Michael and FitzHugh, William and Funke, Roel and Gage, Diane and Harris, Katrina and Heaford, Andrew and Howland, John and Kann, Lisa and Lehoczky, Jessica and LeVine, Rosie and McEwan, Paul and McKernan, Kevin and Meldrim, James and Mesirov, Jill P and Miranda, Cher and Morris, William and Naylor, Jerome and Raymond, Christina and Rosetti, Mark and Santos, Ralph and Sheridan, Andrew and Sougnez, Carrie and Stange-Thomann, Nicole and Stojanovic, Nikola and Subramanian, Aravind and Wyman, Dudley and Rogers, Jane and Sulston, John and Ainscough, Rachael and Beck, Stephan and Bentley, David and Burton, John and Clee, Christopher and Carter, Nigel and Coulson, Alan and Deadman, Rebecca and Deloukas, Panos and Dunham, Andrew and Dunham, Ian and Durbin, Richard and French, Lisa and Grafham, Darren and Gregory, Simon and Hubbard, Tim and Humphray, Sean and Hunt, Adrienne and Jones, Matthew and Lloyd, Christine and McMurray, Amanda and Matthews, Lucy and Mercer, Simon and Milne, Sarah and Mullikin, James C and Mungall, Andrew and Plumb, Robert and Ross, Mark and Shownkeen, Ratna and Sims, Sarah and Waterston, Robert H and Wilson, Richard K and Hillier, LaDeana W and McPherson, John D and Marra, Marco A and Mardis, Elaine R and Fulton, Lucinda A and Chinwalla, Asif T and Pepin, Kymberlie H and Gish, Warren R and Chissoe, Stephanie L and Wendl, Michael C and Delehaunty, Kim D and Miner, Tracie L and Delehaunty, Andrew and Kramer, Jason B and Cook, Lisa L and Fulton, Robert S and Johnson, Douglas L and Minx, Patrick J and Clifton, Sandra W and Hawkins, Trevor and Branscomb, Elbert and Predki, Paul and Richardson, Paul and Wenning, Sarah and Slezak, Tom and Doggett, Norman and Cheng, Jan-Fang and Olsen, Anne and Lucas, Susan and Elkin, Christopher and Uberbacher, Edward and Frazier, Marvin and Gibbs, Richard A and Muzny, Donna M and Scherer, Steven E and Bouck, John B and Sodergren, Erica J and Worley, Kim C and Rives, Catherine M and Gorrell, James H and Metzker, Michael L and Naylor, Susan L and Kucherlapati, Raju S and Nelson, David L and Weinstock, George M and Sakaki, Yoshiyuki and Fujiyama, Asao and Hattori, Masahira and Yada, Tetsushi and Toyoda, Atsushi and Itoh, Takehiko and Kawagoe, Chiharu and Watanabe, Hidemi and Totoki, Yasushi and Taylor, Todd and Weissenbach, Jean and Heilig, Roland and Saurin, William and Artiguenave, Francois and Brottier, Philippe and Bruls, Thomas and Pelletier, Eric and Robert, Catherine and Wincker, Patrick and Rosenthal, Andr{\'{e}} and Platzer, Matthias and Nyakatura, Gerald and Taudien, Stefan and Rump, Andreas and Smith, Douglas R and Doucette-Stamm, Lynn and Rubenfield, Marc and Weinstock, Keith and Lee, Hong Mei and Dubois, JoAnn and Yang, Huanming and Yu, Jun and Wang, Jian and Huang, Guyang and Gu, Jun and Hood, Leroy and Rowen, Lee and Madan, Anup and Qin, Shizen and Davis, Ronald W and Federspiel, Nancy A and Abola, A Pia and Proctor, Michael J and Roe, Bruce A and Chen, Feng and Pan, Huaqin and Ramser, Juliane and Lehrach, Hans and Reinhardt, Richard and McCombie, W Richard and de la Bastide, Melissa and Dedhia, Neilay and Bl{\"{o}}cker, Helmut and Hornischer, Klaus and Nordsiek, Gabriele and Agarwala, Richa and Aravind, L and Bailey, Jeffrey A and Bateman, Alex and Batzoglou, Serafim and Birney, Ewan and Bork, Peer and Brown, Daniel G and Burge, Christopher B and Cerutti, Lorenzo and Chen, Hsiu-Chuan and Church, Deanna and Clamp, Michele and Copley, Richard R and Doerks, Tobias and Eddy, Sean R and Eichler, Evan E and Furey, Terrence S and Galagan, James and Gilbert, James G R and Harmon, Cyrus and Hayashizaki, Yoshihide and Haussler, David and Hermjakob, Henning and Hokamp, Karsten and Jang, Wonhee and Johnson, L Steven and Jones, Thomas A and Kasif, Simon and Kaspryzk, Arek and Kennedy, Scot and Kent, W James and Kitts, Paul and Koonin, Eugene V and Korf, Ian and Kulp, David and Lancet, Doron and Lowe, Todd M and McLysaght, Aoife and Mikkelsen, Tarjei and Moran, John V and Mulder, Nicola and Pollara, Victor J and Ponting, Chris P and Schuler, Greg and Schultz, J{\"{o}}rg and Slater, Guy and Smit, Arian F A and Stupka, Elia and Szustakowki, Joseph and Thierry-Mieg, Danielle and Thierry-Mieg, Jean and Wagner, Lukas and Wallis, John and Wheeler, Raymond and Williams, Alan and Wolf, Yuri I and Wolfe, Kenneth H and Yang, Shiaw-Pyng and Yeh, Ru-Fang and Collins, Francis and Guyer, Mark S and Peterson, Jane and Felsenfeld, Adam and Wetterstrand, Kris A and Myers, Richard M and Schmutz, Jeremy and Dickson, Mark and Grimwood, Jane and Cox, David R and Olson, Maynard V and Kaul, Rajinder and Raymond, Christopher and Shimizu, Nobuyoshi and Kawasaki, Kazuhiko and Minoshima, Shinsei and Evans, Glen A and Athanasiou, Maria and Schultz, Roger and Patrinos, Aristides and Morgan, Michael J},
doi = {10.1038/35057062},
file = {:home/paulo/Documents/work/bib/files/Lander et al. - 2001 - Initial sequencing and analysis of the human genome.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
number = {6822},
pages = {860--921},
title = {{Initial sequencing and analysis of the human genome}},
url = {http://www.nature.com/nature/journal/v409/n6822/full/409860a0.html},
volume = {409},
year = {2001}
}
@article{Li2012,
abstract = {Since the completion of the cucumber and panda genome projects using Illumina sequencing in 2009, the global scientific community has had to pay much more attention to this new cost-effective approach to generate the draft sequence of large genomes. To allow new users to more easily understand the assembly algorithms and the optimum software packages for their projects, we make a detailed comparison of the two major classes of assembly algorithms: overlap-layout-consensus and de-bruijn-graph, from how they match the Lander-Waterman model, to the required sequencing depth and reads length. We also discuss the computational efficiency of each class of algorithm, the influence of repeats and heterozygosity and points of note in the subsequent scaffold linkage and gap closure steps. We hope this review can help further promote the application of second-generation de novo sequencing, as well as aid the future development of assembly algorithms.},
author = {Li, Zhenyu and Chen, Yanxiang and Mu, Desheng and Yuan, Jianying and Shi, Yujian and Zhang, Hao and Gan, Jun and Li, Nan and Hu, Xuesong and Liu, Binghang and Yang, Bicheng and Fan, Wei},
doi = {10.1093/bfgp/elr035},
file = {:home/paulo/Documents/work/bib/files/Li et al. - 2012 - Comparison of the two major classes of assembly algorithms Overlap-layout-consensus and de-bruijn-graph.pdf:pdf},
isbn = {2041-2657 (Electronic)$\backslash$r2041-2649 (Linking)},
issn = {20412649},
journal = {Briefings in Functional Genomics},
keywords = {DBG,De novo assembly,OLC,Second-generation},
number = {1},
pages = {25--37},
pmid = {22184334},
title = {{Comparison of the two major classes of assembly algorithms: Overlap-layout-consensus and de-bruijn-graph}},
volume = {11},
year = {2012}
}
@article{Limasset2016,
abstract = {BACKGROUND Next Generation Sequencing (NGS) has dramatically enhanced our ability to sequence genomes, but not to assemble them. In practice, many published genome sequences remain in the state of a large set of contigs. Each contig describes the sequence found along some path of the assembly graph, however, the set of contigs does not record all the sequence information contained in that graph. Although many subsequent analyses can be performed with the set of contigs, one may ask whether mapping reads on the contigs is as informative as mapping them on the paths of the assembly graph. Currently, one lacks practical tools to perform mapping on such graphs. RESULTS Here, we propose a formal definition of mapping on a de Bruijn graph, analyse the problem complexity which turns out to be NP-complete, and provide a practical solution. We propose a pipeline called GGMAP (Greedy Graph MAPping). Its novelty is a procedure to map reads on branching paths of the graph, for which we designed a heuristic algorithm called BGREAT (de Bruijn Graph REAd mapping Tool). For the sake of efficiency, BGREAT rewrites a read sequence as a succession of unitigs sequences. GGMAP can map millions of reads per CPU hour on a de Bruijn graph built from a large set of human genomic reads. Surprisingly, results show that up to 22 {\%} more reads can be mapped on the graph but not on the contig set. CONCLUSIONS Although mapping reads on a de Bruijn graph is complex task, our proposal offers a practical solution combining efficiency with an improved mapping capacity compared to assembly-based mapping even for complex eukaryotic data.},
archivePrefix = {arXiv},
arxivId = {1505.04911},
author = {Limasset, Antoine and Cazaux, Bastien and Rivals, Eric and Peterlongo, Pierre},
doi = {10.1186/s12859-016-1103-9},
eprint = {1505.04911},
file = {:home/paulo/Documents/work/bib/files/Limasset et al. - 2016 - Read mapping on de Bruijn graphs.pdf:pdf},
isbn = {1285901611039},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Assembly,De Bruijn graph,Genomics,Hamiltonian path,NGS,NP-complete,Read mapping,Sequence graph,path},
number = {1},
pages = {237},
pmid = {27306641},
title = {{Read mapping on de Bruijn graphs.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/27306641{\%}5Cnhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4910249},
volume = {17},
year = {2016}
}
@article{Maass2003,
author = {Maa{\ss}, Moritz},
file = {:home/paulo/Documents/work/bib/files/Maa{\ss} - 2003 - Linear bidirectional on-line construction of affix trees.ps:ps},
journal = {Algorithmica},
number = {1},
pages = {43--74},
title = {{Linear bidirectional on-line construction of affix trees}},
volume = {37},
year = {2003}
}
@article{Manber1993,
abstract = {A new and conceptually simple data structure, called a suffix array, for on-line string searches is introduced in this paper. Constructing and querying suffix arrays is reduced to a sort and search paradigm that employs novel algorithms. The main advantage of suffix arrays over suffix trees is that, in practice, they use three to five times less space. From a complexity standpoint, suffix arrays permit on-line string searches of the type, {\{}“Is{\}} W a substring of A?” to be answered in time {\{}{\$}O(P{\}} + {\{}\backslashtextbackslash{\}}log N){\$}, where P is the length of W and N is the length of A, which is competitive with (and in some cases slightly better than) suffix trees. The only drawback is that in those instances where the underlying alphabet is finite and small, suffix trees can be constructed in {\{}{\$}O(N){\$}{\}} time in the worst case, versus {\{}{\$}O(N{\{}\backslashtextbackslash{\}}log{\}} N){\$} time for suffix arrays. However, an augmented algorithm is given that, regardless of the alphabet size, constructs suffix arrays in {\{}{\$}O(N){\$}expected{\}} time, albeit with lesser space efficiency. It is believed that suffix arrays will prove to be better in practice than suffix trees for many applications., A new and conceptually simple data structure, called a suffix array, for on-line string searches is introduced in this paper. Constructing and querying suffix arrays is reduced to a sort and search paradigm that employs novel algorithms. The main advantage of suffix arrays over suffix trees is that, in practice, they use three to five times less space. From a complexity standpoint, suffix arrays permit on-line string searches of the type, {\{}“Is{\}} W a substring of A?” to be answered in time {\{}{\$}O(P{\}} + {\{}\backslashtextbackslash{\}}log N){\$}, where P is the length of W and N is the length of A, which is competitive with (and in some cases slightly better than) suffix trees. The only drawback is that in those instances where the underlying alphabet is finite and small, suffix trees can be constructed in {\{}{\$}O(N){\$}{\}} time in the worst case, versus {\{}{\$}O(N{\{}\backslashtextbackslash{\}}log{\}} N){\$} time for suffix arrays. However, an augmented algorithm is given that, regardless of the alphabet size, constructs suffix arrays in {\{}{\$}O(N){\$}expected{\}} time, albeit with lesser space efficiency. It is believed that suffix arrays will prove to be better in practice than suffix trees for many applications.},
author = {Manber, Udi and Myers, Gene},
doi = {10.1137/0222058},
file = {:home/paulo/Documents/work/bib/files/Manber, Myers - 1993 - Suffix Arrays A New Method for On-Line String Searches.pdf:pdf},
issn = {0097-5397},
journal = {SIAM Journal on Computing},
number = {5},
pages = {935--948},
shorttitle = {Suffix Arrays},
title = {{Suffix Arrays: A New Method for On-Line String Searches}},
url = {http://epubs.siam.org/doi/abs/10.1137/0222058},
volume = {22},
year = {1993}
}
@article{Mardis2008,
abstract = {Recent scientific discoveries that resulted from the application of next-generation {\{}DNA{\}} sequencing technologies highlight the striking impact of these massively parallel platforms on genetics. These new methods have expanded previously focused readouts from a variety of {\{}DNA{\}} preparation protocols to a genome-wide scale and have fine-tuned their resolution to single base precision. The sequencing of {\{}RNA{\}} also has transitioned and now includes full-length {\{}cDNA{\}} analyses, serial analysis of gene expression ({\{}SAGE)-based{\}} methods, and noncoding {\{}RNA{\}} discovery. Next-generation sequencing has also enabled novel applications such as the sequencing of ancient {\{}DNA{\}} samples, and has substantially widened the scope of metagenomic analysis of environmentally derived samples. Taken together, an astounding potential exists for these technologies to bring enormous change in genetic and biological research and to enhance our fundamental biological knowledge.},
annote = {{\{}PMID:{\}} 18576944},
author = {Mardis, Elaine R},
doi = {10.1146/annurev.genom.9.081307.164359},
file = {:home/paulo/Documents/work/bib/files/Mardis - 2008 - Next-generation DNA sequencing methods.pdf:pdf},
issn = {1527-8204},
journal = {Annual Reviews of Genomics and Human Genetics},
keywords = {Chromatin Immunoprecipitation,Fossils,Gene Expression Profiling,Genome,Genomics,Human,Humans,Sequence Analysis,Untranslated,{\{}DNA{\}},{\{}RNA{\}}},
pages = {387--402},
title = {{Next-generation DNA sequencing methods}},
volume = {9},
year = {2008}
}
@article{Na2003,
abstract = {The suffix tree is a fundamental data structure in the area of string algorithms and it has been used in many applications including data compression. In this paper we propose a data structure called the truncated suffix tree, which is a truncated version of the suffix tree. We also present two linear-time construction algorithms for truncated suffix trees and two algorithms that delete suffixes from truncated suffix trees. The truncated suffix tree is particularly a useful data structure for LZ77 that compresses using a sliding window of a fixed size. Our algorithms lead to two implementations of LZ77 that maintain sliding windows by truncated suffix trees. We also present a technique of finding the longest match in a sliding window, which is a crucial step in LZ77. {\textcopyright} 2003 Elsevier B.V. All rights reserved.},
author = {Na, Joong Chae and Apostolico, Alberto and Iliopoulos, Costas S. and Park, Kunsoo},
doi = {10.1016/S0304-3975(03)00053-7},
isbn = {0304-3975},
issn = {03043975},
journal = {Theoretical Computer Science},
keywords = {Data compression,Data structure,Suffix tree,Text processing},
number = {1-3},
pages = {87--101},
title = {{Truncated suffix trees and their application to data compression}},
volume = {304},
year = {2003}
}
@inproceedings{Navarro2012a,
address = {Bordeaux},
author = {Navarro, Gonzalo and Providel, Eliana},
booktitle = {Proceedings of the 11th international Symposium on Experimental Algorithms, SEA 2012},
doi = {10.1007/978-3-642-30850-5_26},
file = {:home/paulo/Documents/work/bib/files/Navarro, Providel - 2012 - Fast, Small, Simple RankSelect on Bitmaps.pdf:pdf},
isbn = {978-3-642-30849-9},
pages = {295--306},
title = {{Fast, Small, Simple Rank/Select on Bitmaps}},
url = {http://link.springer.com/10.1007/978-3-642-30850-5{\_}26},
year = {2012}
}
@inproceedings{Okanohara2007,
abstract = {Rank/Select dictionaries are data structures for an ordered set S ⊂ {\{}0, 1,..., n − 1{\}} to compute rank(x, S) (the number of elements in S which are no greater than x), and select(i, S) (the i-th smallest element in S), which are the fundamental components of succinct data structures of strings, trees, graphs, etc. In those data structures, however, only asymptotic behavior has been considered and their performance for real data is not satisfactory. In this paper, we propose novel four Rank/Select dictionaries, esp, recrank, vcode and sdarray, each of which is small if the number of elements in S is small, and indeed close to nH0(S) (H0(S) ≤ 1 is the zero-th order empirical entropy of S) in practice, and its query time is superior to the previous ones. Experimental results reveal the characteristics of our data structures and also show that these data structures are superior to existing implementations in both size and query time.},
address = {New Orleans},
archivePrefix = {arXiv},
arxivId = {arXiv:cs/0610001v1},
author = {Okanohara, Daisuke and Sadakane, Kunihiko},
booktitle = {Proceedings of the Ninth Workshop on Algorithm Engineering and Experiments - ALENEX 2007},
eprint = {0610001v1},
file = {:home/paulo/Documents/work/bib/files/Okanohara, Sadakane - 2007 - Practical Entropy-Compressed Rank Select Dictionary.pdf:pdf},
pages = {60--70},
primaryClass = {arXiv:cs},
title = {{Practical Entropy-Compressed Rank / Select Dictionary}},
year = {2007}
}
@article{Pareek2011,
abstract = {The high-throughput - next generation sequencing ({\{}HT-NGS){\}} technologies are currently the hottest topic in the field of human and animals genomics researches, which can produce over 100 times more data compared to the most sophisticated capillary sequencers based on the Sanger method. With the ongoing developments of high throughput sequencing machines and advancement of modern bioinformatics tools at unprecedented pace, the target goal of sequencing individual genomes of living organism at a cost of {\$}1,000 each is seemed to be realistically feasible in the near future. In the relatively short time frame since 2005, the {\{}HT-NGS{\}} technologies are revolutionizing the human and animal genome researches by analysis of chromatin immunoprecipitation coupled to {\{}DNA{\}} microarray ({\{}ChIP-chip){\}} or sequencing ({\{}ChIP-seq){\}}, {\{}RNA{\}} sequencing ({\{}RNA-seq){\}}, whole genome genotyping, genome wide structural variation, de novo assembling and re-assembling of genome, mutation detection and carrier screening, detection of inherited disorders and complex human diseases, {\{}DNA{\}} library preparation, paired ends and genomic captures, sequencing of mitochondrial genome and personal genomics. In this review, we addressed the important features of {\{}HT-NGS{\}} like, first generation {\{}DNA{\}} sequencers, birth of {\{}HT-NGS{\}}, second generation {\{}HT-NGS{\}} platforms, third generation {\{}HT-NGS{\}} platforms: including single molecule Heliscope™, {\{}SMRT™{\}} and {\{}RNAP{\}} sequencers, Nanopore, Archon Genomics X {\{}PRIZE{\}} foundation, comparison of second and third {\{}HT-NGS{\}} platforms, applications, advances and future perspectives of sequencing technologies on human and animal genome research.},
author = {Pareek, Chandra Shekhar and Smoczynski, Rafal and Tretyn, Andrzej},
doi = {10.1007/s13353-011-0057-x},
file = {:home/paulo/Documents/work/bib/files/Pareek, Smoczynski, Tretyn - 2011 - Sequencing technologies and genome sequencing.pdf:pdf},
issn = {1234-1983, 2190-3883},
journal = {Journal of Applied Genetics},
keywords = {Animal Genetics and Genomics,Chip-seq,De novo assembling,High-throughput next generation sequencing,Human Genetics,Life Sciences,Microbial Genetics and Genomics,Personal genomics,Plant Genetics {\&} Genomics,Re-sequencing,general,{\{}CHIP-chip{\}},{\{}RNA-seq{\}}},
number = {4},
pages = {413--435},
title = {{Sequencing technologies and genome sequencing}},
url = {http://link.springer.com/article/10.1007/s13353-011-0057-x},
volume = {52},
year = {2011}
}
@article{Pell2012,
abstract = {Deep sequencing has enabled the investigation of a wide range of environmental microbial ecosystems, but the high memory requirements for de novo assembly of short-read shotgun sequencing data from these complex populations are an increasingly large practical barrier. Here we introduce a memory-efficient graph representation with which we can analyze the k-mer connectivity of metagenomic samples. The graph representation is based on a probabilistic data structure, a Bloom filter, that allows us to efficiently store assembly graphs in as little as 4 bits per k-mer, albeit inexactly. We show that this data structure accurately represents DNA assembly graphs in low memory. We apply this data structure to the problem of partitioning assembly graphs into components as a prelude to assembly, and show that this reduces the overall memory requirements for de novo assembly of metagenomes. On one soil metagenome assembly, this approach achieves a nearly 40-fold decrease in the maximum memory requirements for assembly. This probabilistic graph representation is a significant theoretical advance in storing assembly graphs and also yields immediate leverage on metagenomic assembly.},
archivePrefix = {arXiv},
arxivId = {arXiv:1112.4193v2},
author = {Pell, Jason and Hintze, Arend and Canino-Koning, Rosangela and Howe, Adina and Tiedje, James M and Brown, C. Titus},
doi = {10.1073/pnas.1121464109},
eprint = {arXiv:1112.4193v2},
file = {:home/paulo/Documents/work/bib/files/Pell et al. - 2012 - Scaling metagenome sequence assembly with probabilistic de Bruijn graphs.pdf:pdf},
isbn = {1091-6490 (Electronic)$\backslash$n0027-8424 (Linking)},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
number = {33},
pages = {13272--13277},
pmid = {22847406},
title = {{Scaling metagenome sequence assembly with probabilistic de Bruijn graphs}},
volume = {109},
year = {2012}
}
@article{Pevzner2001,
abstract = {For the last 20 years, fragment assembly in DNA sequencing followed the "overlap-layout-consensus" paradigm that is used in all currently available assembly tools. Although this approach proved useful in assembling clones, it faces difficulties in genomic shotgun assembly. We abandon the classical "overlap-layout-consensus" approach in favor of a new euler algorithm that, for the first time, resolves the 20-year-old "repeat problem" in fragment assembly. Our main result is the reduction of the fragment assembly to a variation of the classical Eulerian path problem that allows one to generate accurate solutions of large-scale sequencing problems. euler, in contrast to the celera assembler, does not mask such repeats but uses them instead as a powerful fragment assembly tool.},
archivePrefix = {arXiv},
arxivId = {arXiv:quant-ph/0209100},
author = {Pevzner, Pavel and Tang, Haixu and Waterman, Michael},
doi = {10.1073/pnas.171285098},
eprint = {0209100},
file = {:home/paulo/Documents/work/bib/files/Pevzner, Tang, Waterman - 2001 - An Eulerian path approach to DNA fragment assembly.pdf:pdf},
isbn = {0027-8424 (Print)},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
keywords = {Algorithms,Bacterial,Bacterial: genetics,Campylobacter jejuni,Campylobacter jejuni: genetics,Contig Mapping,Contig Mapping: methods,DNA,DNA: methods,Genome,Lactococcus lactis,Lactococcus lactis: genetics,Models,Neisseria meningitidis,Neisseria meningitidis: genetics,Sequence Alignment,Sequence Alignment: methods,Sequence Analysis,Software,Theoretical},
month = {aug},
number = {17},
pages = {9748--53},
pmid = {11504945},
primaryClass = {arXiv:quant-ph},
title = {{An Eulerian path approach to DNA fragment assembly.}},
url = {http://www.pnas.org/content/98/17/9748.abstract},
volume = {98},
year = {2001}
}
@article{Pop2008,
abstract = {New {\{}DNA{\}} sequencing technologies can sequence up to one billion bases in a single day at low cost, putting large-scale sequencing within the reach of many scientists. Many researchers are forging ahead with projects to sequence a range of species using the new technologies. However, these new technologies produce read lengths as short as 35–40 nucleotides, posing challenges for genome assembly and annotation. Here we review the challenges and describe some of the bioinformatics systems that are being proposed to solve them. We specifically address issues arising from using these technologies in assembly projects, both de novo and for resequencing purposes, as well as efforts to improve genome annotation in the fragmented assemblies produced by short read lengths.},
author = {Pop, Mihai and Salzberg, Steven L},
doi = {10.1016/j.tig.2007.12.006},
file = {:home/paulo/Documents/work/bib/files/Pop, Salzberg - 2008 - Bioinformatics challenges of new sequencing technology.pdf:pdf},
issn = {0168-9525},
journal = {Trends in Genetics},
number = {3},
pages = {142--149},
title = {{Bioinformatics challenges of new sequencing technology}},
url = {http://www.sciencedirect.com/science/article/pii/S016895250800022X},
volume = {24},
year = {2008}
}
@inproceedings{Raman2002,
abstract = {We consider the {\{}$\backslash$it indexable dictionary{\}} problem, which consists of storing a set S⊆{\{}0,...,m−1{\}} for some integer m, while supporting the operations of {\$}\backslashRank(x){\$}, which returns the number of elements in S that are less than x if x∈S, and -1 otherwise; and {\$}\backslashSelect(i){\$} which returns the i-th smallest element in S. We give a data structure that supports both operations in O(1) time on the RAM model and requires B(n,m)+o(n)+O(lglgm) bits to store a set of size n, where {\$}{\{}\backslashcal B{\}}(n,m) = \backslashceil{\{}\backslashlg {\{}m \backslashchoose n{\}}{\}}{\$} is the minimum number of bits required to store any n-element subset from a universe of size m. Previous dictionaries taking this space only supported (yes/no) membership queries in O(1) time. In the cell probe model we can remove the O(lglgm) additive term in the space bound, answering a question raised by Fich and Miltersen, and Pagh. We present extensions and applications of our indexable dictionary data structure, including: An information-theoretically optimal representation of a k-ary cardinal tree that supports standard operations in constant time, A representation of a multiset of size n from {\{}0,...,m−1{\}} in B(n,m+n)+o(n) bits that supports (appropriate generalizations of) {\$}\backslashRank{\$} and {\$}\backslashSelect{\$} operations in constant time, and A representation of a sequence of n non-negative integers summing up to m in B(n,m+n)+o(n) bits that supports prefix sum queries in constant time.},
address = {San Francisco},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {arXiv:0705.0552v1},
author = {Raman, Rajeev and Raman, Venkatesh and Rao, S Srinivasa},
booktitle = {Proceedings of the thirteenth annual ACM-SIAM symposium on Discrete algorithms - SODA 2002},
eprint = {arXiv:0705.0552v1},
file = {:home/paulo/Documents/work/bib/files/Raman, Raman, Rao - 2002 - Proceedings of the Thirteenth Annual ACM-SIAM Symposium on Discrete Algorithms - SODA'02.pdf:pdf},
pages = {233--242},
title = {{Succinct Indexable Dictionaries with Applications to Encoding k-ary Trees, Prefix Sums and Multisets}},
year = {2002}
}
@article{Rodland2013,
abstract = {BACKGROUND: Processing of reads from high throughput sequencing is often done in terms of edges in the de Bruijn graph representing all k-mers from the reads. The memory requirements for storing all k-mers in a lookup table can be demanding, even after removal of read errors, but can be alleviated by using a memory efficient data structure.$\backslash$n$\backslash$nRESULTS: The FM-index, which is based on the Burrows-Wheeler transform, provides an efficient data structure providing a searchable index of all substrings from a set of strings, and is used to compactly represent full genomes for use in mapping reads to a genome: the memory required to store this is in the same order of magnitude as the strings themselves. However, reads from high throughput sequences mostly have high coverage and so contain the same substrings multiple times from different reads. I here present a modification of the FM-index, which I call the kFM-index, for indexing the set of k-mers from the reads. For DNA sequences, this requires 5 bit of information for each vertex of the corresponding de Bruijn subgraph, i.e. for each different k-1-mer, plus some additional overhead, typically 0.5 to 1 bit per vertex, for storing the equivalent of the FM-index for walking the underlying de Bruijn graph and reproducing the actual k-mers efficiently.$\backslash$n$\backslash$nCONCLUSIONS: The kFM-index could replace more memory demanding data structures for storing the de Bruijn k-mer graph representation of sequence reads. A Java implementation with additional technical documentation is provided which demonstrates the applicability of the data structure (http://folk.uio.no/einarro/Projects/KFM-index/).},
author = {R{\o}dland, Einar Andreas},
doi = {10.1186/1471-2105-14-313},
file = {:home/paulo/Documents/work/bib/files/R{\o}dland - 2013 - Compact representation of k-mer de Bruijn graphs for genome read assembly.pdf:pdf},
isbn = {1471-2105 (Electronic)$\backslash$r1471-2105 (Linking)},
issn = {1471-2105},
journal = {BMC bioinformatics},
number = {1},
pages = {313},
pmid = {24152242},
title = {{Compact representation of k-mer de Bruijn graphs for genome read assembly.}},
url = {http://www.biomedcentral.com/1471-2105/14/313},
volume = {14},
year = {2013}
}
@article{Sacomoto2012,
abstract = {{\{}BACKGROUND:{\}} In this paper, we address the problem of identifying and quantifying polymorphisms in {\{}RNA-seq{\}} data when no reference genome is available, without assembling the full transcripts. Based on the fundamental idea that each polymorphism corresponds to a recognisable pattern in a De Bruijn graph constructed from the {\{}RNA-seq{\}} reads, we propose a general model for all polymorphisms in such graphs. We then introduce an exact algorithm, called {\{}KISSPLICE{\}}, to extract alternative splicing events. {\{}RESULTS:{\}} We show that {\{}KISSPLICE{\}} enables to identify more correct events than general purpose transcriptome assemblers. Additionally, on a 71 M reads dataset from human brain and liver tissues, {\{}KISSPLICE{\}} identified 3497 alternative splicing events, out of which 56{\%} are not present in the annotations, which confirms recent estimates showing that the complexity of alternative splicing has been largely underestimated so far. {\{}CONCLUSIONS:{\}} We propose new models and algorithms for the detection of polymorphism in {\{}RNA-seq{\}} data. This opens the way to a new kind of studies on large {\{}HTS{\}} {\{}RNA-seq{\}} datasets, where the focus is not the global reconstruction of full-length transcripts, but local assembly of polymorphic regions. {\{}KISSPLICE{\}} is available for download at http://alcovna.genouest.org/kissplice/.},
annote = {{\{}PMID:{\}} 22537044 
{\{}PMCID:{\}} {\{}PMC3358658{\}}},
author = {Sacomoto, Gustavo A T and Kielbassa, Janice and Chikhi, Rayan and Uricaru, Raluca and Antoniou, Pavlos and Sagot, Marie-France and Peterlongo, Pierre and Lacroix, Vincent},
doi = {10.1186/1471-2105-13-S6-S5},
file = {:home/paulo/Documents/work/bib/files/Sacomoto et al. - 2012 - KISSPLICE de-novo calling alternative splicing events from RNA-seq data.pdf:pdf},
issn = {1471-2105},
journal = {BMC Bioinformatics},
keywords = {Algorithms,Alternative Splicing,Genome,Humans,Models,Polymorphism,Reference Standards,Sequence Analysis,Single Nucleotide,Statistical,Tandem Repeat Sequences,Transcriptome,{\{}RNA{\}}},
pages = {S5},
shorttitle = {KISSPLICE},
title = {{KISSPLICE: de-novo calling alternative splicing events from RNA-seq data}},
volume = {13 Suppl 6},
year = {2012}
}
@article{Sadakane2007,
abstract = {We introduce new data structures for compressed suffix trees whose size are linear in the text size. The size is measured in bits; thus they occupy only O(n logA) bits for a text of length n on an alphabet A. This is a remarkable improvement...},
author = {Sadakane, Kunihiko},
doi = {10.1007/s00224-006-1198-x},
issn = {14324350},
journal = {Theory of Computing Systems},
number = {4},
pages = {589--607},
title = {{Compressed suffix trees with full functionality}},
volume = {41},
year = {2007}
}
@article{Salikhov2014,
abstract = {De Brujin graphs are widely used in bioinformatics for processing next-generation sequencing data. Due to a very large size of NGS datasets, it is essential to represent de Bruijn graphs compactly, and several approaches to this problem have been proposed recently. In this work, we show how to reduce the memory required by the algorithm of [3] that represents de Brujin graphs using Bloom filters. Our method requires 30{\%} to 40{\%} less memory with respect to the method of [3], with insignificant impact to construction time. At the same time, our experiments showed a better query time compared to [3]. This is, to our knowledge, the best practical representation for de Bruijn graphs.},
archivePrefix = {arXiv},
arxivId = {1302.7278},
author = {Salikhov, Kamil and Sacomoto, Gustavo and Kucherov, Gregory},
eprint = {1302.7278},
file = {:home/paulo/Documents/work/bib/files/Salikhov, Sacomoto, Kucherov - 2014 - Using cascading Bloom filters to improve the memory usage for de Brujin graphs.pdf:pdf},
journal = {Algorithms for Molecular Biology},
keywords = {bloom filter,de brujin graph,genome assembly,next-generation sequencing},
month = {feb},
pages = {2},
title = {{Using cascading Bloom filters to improve the memory usage for de Brujin graphs}},
url = {http://arxiv.org/abs/1302.7278},
volume = {9},
year = {2014}
}
@article{Schnattinger2012,
abstract = {Searching for genes encoding microRNAs (miRNAs) is an important task in genome analysis. Because the secondary structure of miRNA (but not the sequence) is highly conserved, the genes encoding it can be determined by finding regions in a genomic DNA sequence that match the structure. It is known that algorithms using a bidirectional search on the DNA sequence for this task outperform algorithms based on unidirectional search. The data structures supporting a bidirectional search (affix trees and affix arrays), however, are rather complex and suffer from their large space consumption. Here, we present a new data structure called bidirectional wavelet index that supports bidirectional search with much less space. With this data structure, it is possible to search for candidates of RNA secondary structural patterns in large genomes, for example the complete human genome. Another important application of this data structure is short read alignment. As a second contribution, we show how bidirectional matching statistics can be computed in linear time. ?? 2012 Elsevier Inc. All rights reserved.},
author = {Schnattinger, Thomas and Ohlebusch, Enno and Gog, Simon},
doi = {10.1016/j.ic.2011.03.007},
file = {:home/paulo/Documents/work/bib/files/Schnattinger, Ohlebusch, Gog - 2012 - Bidirectional search in a string with wavelet trees and bidirectional matching statistics.pdf:pdf},
isbn = {3642135080},
issn = {08905401},
journal = {Information and Computation},
keywords = {Bidirectional search,Matching statistics,String matching},
pages = {13--22},
title = {{Bidirectional search in a string with wavelet trees and bidirectional matching statistics}},
volume = {213},
year = {2012}
}
@article{Simpson2009,
abstract = {Widespread adoption of massively parallel deoxyribonucleic acid ({\{}DNA){\}} sequencing instruments has prompted the recent development of de novo short read assembly algorithms. A common shortcoming of the available tools is their inability to efficiently assemble vast amounts of data generated from large-scale sequencing projects, such as the sequencing of individual human genomes to catalog natural genetic variation. To address this limitation, we developed {\{}ABySS{\}} (Assembly By Short Sequences), a parallelized sequence assembler. As a demonstration of the capability of our software, we assembled 3.5 billion paired-end reads from the genome of an African male publicly released by Illumina, Inc. Approximately 2.76 million contigs ≥100 base pairs (bp) in length were created with an N50 size of 1499 bp, representing 68{\%} of the reference human genome. Analysis of these contigs identified polymorphic and novel sequences not present in the human reference assembly, which were validated by alignment to alternate human assemblies and to other primate genomes.},
annote = {{\{}PMID:{\}} 19251739},
author = {Simpson, Jared T and Wong, Kim and Jackman, Shaun D and Schein, Jacqueline E and Jones, Steven J M and Birol, İnan{\c{c}}},
doi = {10.1101/gr.089532.108},
file = {:home/paulo/Documents/work/bib/files/Simpson et al. - 2009 - ABySS A parallel assembler for short read sequence data.pdf:pdf},
issn = {1088-9051, 1549-5469},
journal = {Genome Research},
pages = {1117--1123},
shorttitle = {ABySS},
title = {{ABySS: A parallel assembler for short read sequence data}},
url = {http://genome.cshlp.org/content/19/6/1117},
volume = {19},
year = {2009}
}
@article{Strothmann2007,
abstract = {Efficient string-processing in large data sets like complete genomes is strongly connected to the suffix tree and similar index data structures. With respect to complex structural string analysis like the search for RNA secondary structure patterns, unidirectional suffix tree algorithms are inferior to bidirectional algorithms based on the affix tree data structure. The affix tree incorporates the suffix tree and the suffix tree of the reverse text in one tree structure but suffers from its large memory requirements. In this paper I present a new data structure, denoted affix array, which is equivalent to the affix tree with respect to its algorithmic functionality, but with smaller memory requirements and improved performance. I will show a linear time construction of the affix array without making use of the linear time construction of the affix tree. I will also show how bidirectional affix tree traversals can be transferred to the affix array and present the impressive results of large scale RNA secondary structure analysis based on the new data structure. {\textcopyright} 2007 Elsevier Ltd. All rights reserved.},
author = {Strothmann, Dirk},
doi = {10.1016/j.tcs.2007.09.029},
file = {:home/paulo/Documents/work/bib/files/Strothmann - 2007 - The affix array data structure and its applications to RNA secondary structure analysis.pdf:pdf},
issn = {03043975},
journal = {Theoretical Computer Science},
keywords = {Affix tree,Pattern matching,Suffix array,Suffix tree},
number = {1-2},
pages = {278--294},
title = {{The affix array data structure and its applications to RNA secondary structure analysis}},
volume = {389},
year = {2007}
}
@article{Venter2001,
abstract = {A 2.91-billion base pair (bp) consensus sequence of the euchromatic portion of the human genome was generated by the whole-genome shotgun sequencing method. The 14.8-billion bp {\{}DNA{\}} sequence was generated over 9 months from 27,271,853 high-quality sequence reads (5.11-fold coverage of the genome) from both ends of plasmid clones made from the {\{}DNA{\}} of five individuals. Two assembly strategies—a whole-genome assembly and a regional chromosome assembly—were used, each combining sequence data from Celera and the publicly funded genome effort. The public data were shredded into 550-bp segments to create a 2.9-fold coverage of those genome regions that had been sequenced, without including biases inherent in the cloning and assembly procedure used by the publicly funded group. This brought the effective coverage in the assemblies to eightfold, reducing the number and size of gaps in the final assembly over what would be obtained with 5.11-fold coverage. The two assembly strategies yielded very similar results that largely agree with independent mapping data. The assemblies effectively cover the euchromatic regions of the human chromosomes. More than 90{\%} of the genome is in scaffold assemblies of 100,000 bp or more, and 25{\%} of the genome is in scaffolds of 10 million bp or larger. Analysis of the genome sequence revealed 26,588 protein-encoding transcripts for which there was strong corroborating evidence and an additional ∼12,000 computationally derived genes with mouse matches or other weak supporting evidence. Although gene-dense clusters are obvious, almost half the genes are dispersed in low {\{}G+C{\}} sequence separated by large tracts of apparently noncoding sequence. Only 1.1{\%} of the genome is spanned by exons, whereas 24{\%} is in introns, with 75{\%} of the genome being intergenic {\{}DNA.{\}} Duplications of segmental blocks, ranging in size up to chromosomal lengths, are abundant throughout the genome and reveal a complex evolutionary history. Comparative genomic analysis indicates vertebrate expansions of genes associated with neuronal function, with tissue-specific developmental regulation, and with the hemostasis and immune systems. {\{}DNA{\}} sequence comparisons between the consensus sequence and publicly funded genome data provided locations of 2.1 million single-nucleotide polymorphisms ({\{}SNPs).{\}} A random pair of human haploid genomes differed at a rate of 1 bp per 1250 on average, but there was marked heterogeneity in the level of polymorphism across the genome. Less than 1{\%} of all {\{}SNPs{\}} resulted in variation in proteins, but the task of determining which {\{}SNPs{\}} have functional consequences remains an open challenge.},
annote = {{\{}PMID:{\}} 11181995},
author = {Venter, J Craig and Adams, Mark D and Myers, Eugene W and Li, Peter W and Mural, Richard J and Sutton, Granger G and Smith, Hamilton O and Yandell, Mark and Evans, Cheryl A and Holt, Robert A and Gocayne, Jeannine D and Amanatides, Peter and Ballew, Richard M and Huson, Daniel H and Wortman, Jennifer Russo and Zhang, Qing and Kodira, Chinnappa D and Zheng, Xiangqun H and Chen, Lin and Skupski, Marian and Subramanian, Gangadharan and Thomas, Paul D and Zhang, Jinghui and Miklos, George L Gabor and Nelson, Catherine and Broder, Samuel and Clark, Andrew G and Nadeau, Joe and McKusick, Victor A and Zinder, Norton and Levine, Arnold J and Roberts, Richard J and Simon, Mel and Slayman, Carolyn and Hunkapiller, Michael and Bolanos, Randall and Delcher, Arthur and Dew, Ian and Fasulo, Daniel and Flanigan, Michael and Florea, Liliana and Halpern, Aaron and Hannenhalli, Sridhar and Kravitz, Saul and Levy, Samuel and Mobarry, Clark and Reinert, Knut and Remington, Karin and Abu-Threideh, Jane and Beasley, Ellen and Biddick, Kendra and Bonazzi, Vivien and Brandon, Rhonda and Cargill, Michele and Chandramouliswaran, Ishwar and Charlab, Rosane and Chaturvedi, Kabir and Deng, Zuoming and Francesco, Valentina Di and Dunn, Patrick and Eilbeck, Karen and Evangelista, Carlos and Gabrielian, Andrei E and Gan, Weiniu and Ge, Wangmao and Gong, Fangcheng and Gu, Zhiping and Guan, Ping and Heiman, Thomas J and Higgins, Maureen E and Ji, Rui-Ru and Ke, Zhaoxi and Ketchum, Karen A and Lai, Zhongwu and Lei, Yiding and Li, Zhenya and Li, Jiayin and Liang, Yong and Lin, Xiaoying and Lu, Fu and Merkulov, Gennady V and Milshina, Natalia and Moore, Helen M and Naik, Ashwinikumar K and Narayan, Vaibhav A and Neelam, Beena and Nusskern, Deborah and Rusch, Douglas B and Salzberg, Steven and Shao, Wei and Shue, Bixiong and Sun, Jingtao and Wang, Zhen Yuan and Wang, Aihui and Wang, Xin and Wang, Jian and Wei, Ming-Hui and Wides, Ron and Xiao, Chunlin and Yan, Chunhua and Yao, Alison and Ye, Jane and Zhan, Ming and Zhang, Weiqing and Zhang, Hongyu and Zhao, Qi and Zheng, Liansheng and Zhong, Fei and Zhong, Wenyan and Zhu, Shiaoping C and Zhao, Shaying and Gilbert, Dennis and Baumhueter, Suzanna and Spier, Gene and Carter, Christine and Cravchik, Anibal and Woodage, Trevor and Ali, Feroze and An, Huijin and Awe, Aderonke and Baldwin, Danita and Baden, Holly and Barnstead, Mary and Barrow, Ian and Beeson, Karen and Busam, Dana and Carver, Amy and Center, Angela and Cheng, Ming Lai and Curry, Liz and Danaher, Steve and Davenport, Lionel and Desilets, Raymond and Dietz, Susanne and Dodson, Kristina and Doup, Lisa and Ferriera, Steven and Garg, Neha and Gluecksmann, Andres and Hart, Brit and Haynes, Jason and Haynes, Charles and Heiner, Cheryl and Hladun, Suzanne and Hostin, Damon and Houck, Jarrett and Howland, Timothy and Ibegwam, Chinyere and Johnson, Jeffery and Kalush, Francis and Kline, Lesley and Koduru, Shashi and Love, Amy and Mann, Felecia and May, David and McCawley, Steven and McIntosh, Tina and McMullen, Ivy and Moy, Mee and Moy, Linda and Murphy, Brian and Nelson, Keith and Pfannkoch, Cynthia and Pratts, Eric and Puri, Vinita and Qureshi, Hina and Reardon, Matthew and Rodriguez, Robert and Rogers, Yu-Hui and Romblad, Deanna and Ruhfel, Bob and Scott, Richard and Sitter, Cynthia and Smallwood, Michelle and Stewart, Erin and Strong, Renee and Suh, Ellen and Thomas, Reginald and Tint, Ni Ni and Tse, Sukyee and Vech, Claire and Wang, Gary and Wetter, Jeremy and Williams, Sherita and Williams, Monica and Windsor, Sandra and Winn-Deen, Emily and Wolfe, Keriellen and Zaveri, Jayshree and Zaveri, Karena and Abril, Josep F and Guig{\'{o}}, Roderic and Campbell, Michael J and Sjolander, Kimmen V and Karlak, Brian and Kejariwal, Anish and Mi, Huaiyu and Lazareva, Betty and Hatton, Thomas and Narechania, Apurva and Diemer, Karen and Muruganujan, Anushya and Guo, Nan and Sato, Shinji and Bafna, Vineet and Istrail, Sorin and Lippert, Ross and Schwartz, Russell and Walenz, Brian and Yooseph, Shibu and Allen, David and Basu, Anand and Baxendale, James and Blick, Louis and Caminha, Marcelo and Carnes-Stine, John and Caulk, Parris and Chiang, Yen-Hui and Coyne, My and Dahlke, Carl and Mays, Anne Deslattes and Dombroski, Maria and Donnelly, Michael and Ely, Dale and Esparham, Shiva and Fosler, Carl and Gire, Harold and Glanowski, Stephen and Glasser, Kenneth and Glodek, Anna and Gorokhov, Mark and Graham, Ken and Gropman, Barry and Harris, Michael and Heil, Jeremy and Henderson, Scott and Hoover, Jeffrey and Jennings, Donald and Jordan, Catherine and Jordan, James and Kasha, John and Kagan, Leonid and Kraft, Cheryl and Levitsky, Alexander and Lewis, Mark and Liu, Xiangjun and Lopez, John and Ma, Daniel and Majoros, William and McDaniel, Joe and Murphy, Sean and Newman, Matthew and Nguyen, Trung and Nguyen, Ngoc and Nodell, Marc and Pan, Sue and Peck, Jim and Peterson, Marshall and Rowe, William and Sanders, Robert and Scott, John and Simpson, Michael and Smith, Thomas and Sprague, Arlan and Stockwell, Timothy and Turner, Russell and Venter, Eli and Wang, Mei and Wen, Meiyuan and Wu, David and Wu, Mitchell and Xia, Ashley and Zandieh, Ali and Zhu, Xiaohong},
doi = {10.1126/science.1058040},
file = {:home/paulo/Documents/work/bib/files/Venter et al. - 2001 - The Sequence of the Human Genome.pdf:pdf},
issn = {0036-8075, 1095-9203},
journal = {Science},
number = {5507},
pages = {1304--1351},
title = {{The Sequence of the Human Genome}},
url = {http://www.sciencemag.org/content/291/5507/1304},
volume = {291},
year = {2001}
}
@inproceedings{Vigna2008,
abstract = {Research on succinct data structures (data structures occupying space close to the information-theoretical lower bound, but achieving speed similar to their standard counterparts) has steadily increased in the last few years. However, many theoretical constructions providing asymptotically optimal bounds are unusable in practise because of the very large constants involved. The study of practical implementations of the basic building blocks of such data structures is thus fundamental to obtain practical applications. In this paper we argue that 64-bit and wider architectures are particularly suited to very efficient implementations of rank (counting the number of ones up to a given position) and select (finding the position of the i -th bit set), two essential building blocks of all succinct data structures. Contrarily to typical 32-bit approaches, involving precomputed tables, we use pervasively broadword (a.k.a. SWAR--"SIMD in A Register") programming, which compensates the constant burden associated to succinct structures by solving problems in parallel in a register. We provide an implementation named rank9 that addresses 264 bits, consumes less space and is significantly faster then current state-of-the-art 32-bit implementations, and a companion select9 structure that selects in nearly constant time using only access to aligned data. For sparsely populated arrays, we provide a simple broadword implementation of the Elias-Fano representation of monotone sequences. In doing so, we develop broadword algorithms for performing selection in a word or in a sequence of words that are of independent interest.},
address = {Provincetown},
author = {Vigna, Sebastiano},
booktitle = {Proceedings of the 7th international conference on Experimental algorithms - WEA'08},
doi = {10.1007/978-3-540-68552-4},
file = {:home/paulo/Documents/work/bib/files/Vigna - 2008 - Broadword Implementation of Rank Select Queries.pdf:pdf},
isbn = {9783540685487},
keywords = {Rank/Select},
pages = {1--11},
title = {{Broadword Implementation of Rank / Select Queries}},
url = {http://sux.dsi.unimi.it/paper.pdf},
year = {2008}
}
@article{Watson1953,
annote = {{\{}PMID:{\}} 13054692},
author = {Watson, James D and Crick, Francis H},
file = {:home/paulo/Documents/work/bib/files/Watson, Crick - 1953 - Molecular structure of nucleic acids a structure for deoxyribose nucleic acid.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
keywords = {Nucleic Acids},
number = {4356},
pages = {737--738},
title = {{Molecular structure of nucleic acids; a structure for deoxyribose nucleic acid}},
volume = {171},
year = {1953}
}
@article{Zerbino2008,
abstract = {We have developed a new set of algorithms, collectively called {\{}“Velvet{\}},” to manipulate de Bruijn graphs for genomic sequence assembly. A de Bruijn graph is a compact representation based on short words (k-mers) that is ideal for high coverage, very short read (25–50 bp) data sets. Applying Velvet to very short reads and paired-ends information only, one can produce contigs of significant length, up to 50-kb N50 length in simulations of prokaryotic data and 3-kb N50 on simulated mammalian {\{}BACs.{\}} When applied to real Solexa data sets without read pairs, Velvet generated contigs of ∼8 kb in a prokaryote and 2 kb in a mammalian {\{}BAC{\}}, in close agreement with our simulated results without read-pair information. Velvet represents a new approach to assembly that can leverage very short reads in combination with read pairs to produce useful assemblies.},
annote = {{\{}PMID:{\}} 18349386},
author = {Zerbino, Daniel R and Birney, Ewan},
doi = {10.1101/gr.074492.107},
file = {:home/paulo/Documents/work/bib/files/Zerbino, Birney - 2008 - Velvet Algorithms for de novo short read assembly using de Bruijn graphs.pdf:pdf},
issn = {1088-9051},
journal = {Genome Research},
month = {feb},
number = {5},
pages = {821--829},
shorttitle = {Velvet},
title = {{Velvet: Algorithms for de novo short read assembly using de Bruijn graphs}},
url = {http://genome.cshlp.org/content/18/5/821 http://www.genome.org/cgi/doi/10.1101/gr.074492.107},
volume = {18},
year = {2008}
}
